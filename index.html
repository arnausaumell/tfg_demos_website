<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>TFG Demo</title>
  
  <meta name="author" content="Arnau Saumell">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/whatever_image.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; margin-bottom:80px;">
    <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <h1 font-size=12px>Deformable object visual perception for robotic manipulation</h1>
        <p style="font-size:15px; margin-bottom:30px;">
          <i>This is a support website to provide additional graphical content for my Final Bachelor Thesis on "Deformable object visual perception for robotic manipulation". This work was elaborated in collaboration with the Massachusetts Institute of Technology (MIT) and Universitat Polit√®cnica de Catalunya (UPC).</i>
        </p>
        <p>
          <b>Abstract</b>: How can we achieve a dense characterization of a highly deformable object for robotic manipulation using visual perception? The infinite dimensional state space of deformable objects makes this task extremely difficult. Moreover, due to the nature of these objects, dealing with self-occlusions becomes a second major challenge. The frequency and complexity of these, increase with the degree of deformability of the object. Thus, it is crucial to properly deal with them. This thesis proposes a method that, by using visual perception (RGB images), (1) provides a task-agnostic dense representation of an object class, (2) enables to generalize such a representation across different instances of the same object category, and (3) provides a measure of confidence on pixelwise correspondence matching. In this particular case, the focus is on cloth manipulation, more specifically on instances of shirts. Hence, this work also provides a pipeline to generate a simulated dataset of randomized instances within this object category. However, the whole framework could be generalized to any deformable object.
        </p>


        <h2 font-zie=12px>Simulation dataset generation</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:50px"><tbody>
            <tr>
            <td style="padding-left:20px;padding-right:20px;padding-bottom:20px;width:100%;vertical-align:middle;text-align:justify;">
              <heading>Simulation of a hanging motion</heading>
              <p>
                This video shows a simulation of how the motion to create shirts hanging from a single grasping point are created. The rendering program used to create them was Blender.
              </p>
              <video style="width:60%; margin-left:20%; margin-right:20%;padding-top: 10px;" controls>
                <source src="videos/shirt_simulation.mp4" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody></table>

        <h2 font-zie=12px>Real world experiments</h2>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:20px;padding-right:20px;padding-bottom:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading>Test 1: red T-shirt</heading>
              <p>
                This first example, shows a basic grasping motion on a selected point of the canonical shirt image. In this case, the correspondence is correctly found and the grasp is done successfully. The last part of the video shows an evaluation of the performance of the model for the given image. The estimated probability distribution clearly concentrates the mass around the right part of the shirt. However, we can see some probability being assigned to the tip of one of the sleeves, which stands for a common mode of failure (distinguishing sleeves vs torso).
              </p>

              <video style="width:80%; margin-left:10%; margin-right:10%;padding-top: 10px;" controls>
                <source src="videos/video1.mp4" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading>Test 2: red T-shirt - failure mode</heading>
              <p>
                In this second example, using the same piece of cloth than in Test 1, we can see a case of failure of our correspondence model. We can see how a point over the torso is mistaken to be the shirt neck. This clearly happens because the whole representation of the object is wrong. However, from the outputted probability distribution we can extract two interesting facts. Firstly, the second main mode over this distribution is correctly place over the shirt neck. Secondly, the distribution in this case is quite plain and spread around all the cloth. Hence, in this case, our confidence scorer would classify this prediction as a low-confident one.
              </p>

              <video style="width:80%; margin-left:10%; margin-right:10%;padding-top: 10px;" controls>
                <source src="videos/video2.mp4" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading>Test 3: purple jumper</heading>
              <p>
                In order to test the capacity of our model to generalize to other upper-body cloth instances, in this third example we are testing with a purple jumper. This case shows a nice test in which the sleeve of the cloth is correctly found and represented through our dense object descriptor representation. Hence, the predicted probability distribution correctly locates the probability mass around the tip of both sleeves. 
              </p>

              <video style="width:80%; margin-left:10%; margin-right:10%;padding-top: 10px;" controls>
                <source src="videos/video3.mp4" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading>Test 4: changing lightning conditions</heading>
              <p>
                Finally, we provide a last test to proof our model's robustness to changing lightning conditions. To do so, we take once again the red T-shirt but now apply a new lightning setup. Despite correctly finding the pixelwise correspondence for the selected pixel, our most common mode of failure reappears. Once again, as shown in the Dense Object Description of our object, the lowest part of the torso is confused to be too similar to a sleeve. However, the biggest probability mode is still predicted over the correct area of the cloth.
              </p>

              <video style="width:80%; margin-left:10%; margin-right:10%;padding-top: 10px;" controls>
                <source src="videos/video4.mp4" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody></table>
        


      </td>
    </tr>
  </table>
</body>

</html>